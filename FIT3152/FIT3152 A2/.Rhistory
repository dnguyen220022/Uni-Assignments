pdBest <- randomForest(Class~., data = PD.trainOmit, ntree = 300, mtry = 2, sampsize = c('0' = 300, '1' = 160), replace = TRUE)
pdBestPredict <- predict(pdBest, PD.testOmit, type = 'prob')[,2] %>%
prediction(., PD.testOmit$Class)
pdBestPerf <- performance(pdBestPredict, "tpr", "fpr")
performance(pdBestPredict, "auc")@y.values
pdBestPredict <- predict(pdBest, PD.testOmit)
table(predicted = pdBestPredict, actual = PD.testOmit$Class)
pdBestPredict <- predict(pdBest, PD.testOmit, type = 'prob')[,2] %>%
prediction(., PD.testOmit$Class)
pdBestPerf <- performance(pdBestPredict, "tpr", "fpr")
performance(pdBestPredict, "auc")@y.values
library(neuralnet)
nn.train <- na.omit(PD.train)
nn.test <- na.omit(PD.test)
nn.train <- as.data.frame(scale(nn.train[1:25]))
nn.train$Class <- (na.omit(PD.train))$Class
nn.test <- as.data.frame(scale(nn.test[1:25]))
nn.test$Class <- (na.omit(PD.test))$Class
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A18 + A22 + A23 + A24, nn.train, hidden = 5, stepmax = 1e+10)
plot(pd.nn)
nn.pred <- compute(pd.nn, nn.test)
nn.predr
nn.pred
View(nn.pred)
View(nn.pred)
nn.pred$net.result
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predr
nn.predrdf <- as.data.frame(as.table(nn.predr))
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A18 + A22 + A23 + A24, nn.train, hidden = c(5,5), stepmax = 1e+10)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A18 + A22 + A23 + A24, nn.train, hidden = 5, stepmax = 1e+10)
plot(pd.nn)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A18 + A22 + A23 + A24, nn.train, hidden = 6, stepmax = 1e+10)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#confusion matrix
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A18 + A22 + A23 + A24, nn.train, hidden = 7, stepmax = 1e+10)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#confusion matrix
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = 6, stepmax = 1e+10)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#confusion matrix
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A18 + A22 + A23 + A24, nn.train, hidden = 5, stepmax = 1e+10)
#prediction + prediction processing
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#confusion matrix
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A18 + A22 + A23 + A24, nn.train, hidden = 5, stepmax = 1e+10)
#prediction + prediction processing
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#confusion matrix
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = 6, stepmax = 1e+10)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#confusion matrix
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
?predict
nnPredict <- nn.pred[,2]
nn.pred
plot(pd.nn)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
nnPredict <- nn.pred$net.result[, 2] %>%
stats::prediction(., PD.testOmit$Class)
nnPredict <- nn.pred$net.result[, 2] %>%
ROCR::prediction(., PD.testOmit$Class)
nnPerf <- performance(nnPredict, "tpr", "fpr")
performance(pdBestPredict, "auc")@y.values
plot(nnPerf, main = "ROC Curve of Neural Network", col = "blue")
performance(pdBestPredict, "auc")@y.values
plot(nnPerf, main = "ROC Curve of Neural Network", col = "blue")
plot(pd.nn)
plot(nnPerf, main = "ROC Curve of Neural Network", col = "blue")
plot(nnPerf, main = "ROC Curve of Neural Network", col = "blue")
lines(c(0, 1), c(0, 1), col = "black", lty = 2)
plot(nnPerf, main = "ROC Curve of Neural Network", col = "blue")
lines(c(0, 1), c(0, 1), col = "black", lty = 2)
legend("bottomright", legend = c("Neural Network"), col = c("blue"), lwd = 2)
detach("package:neuralnet:, unload + TRUE)
#decision tree
pdDT <- tree(Class~., data = PD.train)
#naive bayes
pdNB <- naiveBayes(Class~., data = PD.train)
#bagging
#take sample with replacement
#see https://www.ibm.com/topics/bagging
set.seed(32471033)
sub <-c(sample(1:1400, 700, replace = TRUE))
#fit model
pdBag <- bagging(Class~., data = PD.train[sub,])
#boosting
pdBoost <- boosting(Class~., data = PD.train)
#random forest
pdRF <- randomForest(Class~., data = PD.train, na.action = na.exclude)
pdDTPredict <- predict(pdDT, PD.test)[,2] %>%
prediction(., PD.test$Class)
pdDTPerf <- performance(pdDTPredict, "tpr", "fpr")
pdDTPredict <- predict(pdDT, PD.test)[,2] %>%
prediction(., PD.test$Class)
pdDT <- tree(Class~., data = PD.train)
#naive bayes
pdNB <- naiveBayes(Class~., data = PD.train)
#bagging
#take sample with replacement
#see https://www.ibm.com/topics/bagging
set.seed(32471033)
sub <-c(sample(1:1400, 700, replace = TRUE))
#fit model
pdBag <- bagging(Class~., data = PD.train[sub,])
#boosting
pdBoost <- boosting(Class~., data = PD.train)
#random forest
pdRF <- randomForest(Class~., data = PD.train, na.action = na.exclude)
pdDTPredict <- predict(pdDT, PD.test)[,2] %>%
prediction(., PD.test$Class)
pdDTPredict <- predict(pdDT, PD.test)[,2] %>%
ROCR::prediction(., PD.test$Class)
pdDTPerf <- performance(pdDTPredict, "tpr", "fpr")
#naive bayes
pdNBPredict <- predict(pdNB, PD.test, type = 'raw')[,2] %>%
ROCR::prediction(., PD.test$Class)
pdNBPerf <- performance(pdNBPredict, "tpr", "fpr")
#bagging
pdBagPredict <- predict(pdBag, PD.test)$prob[,2] %>%
ROCR::prediction(., PD.test$Class)
pdBagPerf <- performance(pdBagPredict, "tpr", "fpr")
#boosting
pdBoostPredict <- predict(pdBoost, PD.test)$prob[,2] %>%
ROCR::prediction(., PD.test$Class)
pdBoostPerf <- performance(pdBoostPredict, "tpr", "fpr")
#random forest
pdRFPredict <- na.omit(predict(pdRF, PD.test, type = 'prob'))[,2] %>%
ROCR::prediction(., na.omit(PD.test)$Class)
pdRFPerf <- performance(pdRFPredict, "tpr", "fpr")
plot(pdDTPerf, main = "ROC Curve", col = "blue")
plot(pdNBPerf, col = "red", add = TRUE)
plot(pdBagPerf, col = "green", add = TRUE)
plot(pdBoostPerf, col = "pink", add = TRUE)
plot(pdRFPerf, col = "yellow", add = TRUE)
plot(nnPerf, col = "purple", add = TRUE)
lines(c(0, 1), c(0, 1), col = "black", lty = 2)
legend("bottomright", legend = c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "Neural Network"), col = c("blue", "red", "green", "pink", "yellow", "purple"), lwd = 2)
summary(Phish$A17)
summary(Phish$17, sum)
summary(Phish$17, count)
summary(Phish$17, count)
table(Phish$A17)
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(3, 3), stepmax = 1e+10)
plot(pd.nn)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
nnPredict <- nn.pred$net.result[, 2] %>%
ROCR::prediction(., PD.testOmit$Class)
nnPerf <- performance(nnPredict, "tpr", "fpr")
performance(pdBestPredict, "auc")@y.values
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(4, 4), stepmax = 1e+10)
plot(pd.nn)
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
nnPredict <- nn.pred$net.result[, 2] %>%
ROCR::prediction(., PD.testOmit$Class)
nnPerf <- performance(nnPredict, "tpr", "fpr")
performance(pdBestPredict, "auc")@y.values
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(3, 4), stepmax = 1e+10)
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(3, 4), stepmax = 1e+10, lifesign = "minimal")
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(3, 4), stepmax = 1e+10, lifesign = "full")
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(3, 4), stepmax = 1e+5, lifesign = "full")
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(6), stepmax = 1e+6, lifesign = "full")
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#ROC processing
nnPredict <- nn.pred$net.result[, 2] %>%
ROCR::prediction(., PD.testOmit$Class)
nnPerf <- performance(nnPredict, "tpr", "fpr")
performance(pdBestPredict, "auc")@y.values
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
set.seed(32471033)
pd.nn <- neuralnet(Class~A01 + A08 + A12 + A14 + A17 + A18 + A22 + A23 + A24, nn.train, hidden = c(6), stepmax = 1e+6, lifesign = "full")
nn.pred <- compute(pd.nn, nn.test)
nn.predr <- round(nn.pred$net.result[, 2], 0)
nn.predrdf <- as.data.frame(as.table(nn.predr))
#ROC processing
nnPredict <- nn.pred$net.result[, 2] %>%
ROCR::prediction(., PD.testOmit$Class)
nnPerf <- performance(nnPredict, "tpr", "fpr")
#plot ROC curve
plot(pdDTPerf, main = "ROC Curve", col = "blue")
plot(pdNBPerf, col = "red", add = TRUE)
plot(pdBagPerf, col = "green", add = TRUE)
plot(pdBoostPerf, col = "pink", add = TRUE)
plot(pdRFPerf, col = "yellow", add = TRUE)
plot(nnPerf, col = "purple", add = TRUE)
lines(c(0, 1), c(0, 1), col = "black", lty = 2)
legend("bottomright", legend = c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "Neural Network"), col = c("blue", "red", "green", "pink", "yellow", "purple"), lwd = 2)
#get AUC
performance(pdBestPredict, "auc")@y.values
#confusion matrix
table(predicted = nn.predrdf$Freq, actual = nn.test$Class)
performance(pdDTPredict, "auc")@y.values
performance(pdNBPredict, "auc")@y.values
performance(pdBagPredict, "auc")@y.values
performance(pdBoostPredict, "auc")@y.values
performance(pdRFPredict, "auc")@y.values
install.packages("less")
library(less)
SVC$fit(PD[, -26], PD$Class)
svc <- SVC$new()
svc$fit(PD[, -26], PD$Class)
svc <- SVC$new
svcPred <- svc$fit(PD.train[, -26], PD.train$Class)$predict(PD.test)
data(iris)
split_list <- train_test_split(iris, test_size =  0.3)
X_train <- split_list[[1]]
X_test <- split_list[[2]]
y_train <- split_list[[3]]
y_test <- split_list[[4]]
View(X_test)
View(y_test)
svc <- SVC$new
svcPred <- svc$fit(PD.train[, -26], PD.train$Class)$predict(PD.test[, -26])
svcPred <- svc$fit(PD.train[, -26], PD.train$Class)
svc <- SVC$new()
svc$fit(X_train, y_train)
preds <- svc$predict(X_test)
preds
print(caret::confusionMatrix(data=preds, reference = factor(y_test)))
x.train <- PD.train[, -26]
View(x.train)
x.train <- PD.train[, -26]
y.train <- PD.train$Class
x.test <- PD.test[, -26]
y.test <- PD.test$Class
svc <- SVC$new()
svc$fit(X_train, y_train)
preds <- svc$predict(X_test)
preds
svc <- SVC$new()
svc$fit(x.train, y.train)
preds <- svc$predict(x.test)
preds
preds <- as.data.frame(svc$predict(x.test))
preds
svcPreds <- as.data.frame(svc$predict(x.test))
table(predicted = svcPreds, actual = PD.test$Class)
PD.test$Class
svcPreds <- as.matrix(svc$predict(x.test))
svcPreds
table(predicted = svcPreds, actual = PD.test$Class)
svcTrain <- na.omit(PD.train)
svcTest <- na.omit(PD.test)
x.train <- svcTrain[, -26]
y.train <- svcTrain$Class
x.test <- svcTest[, -26]
y.test <- svcTest$Class
svc <- SVC$new()
svc$fit(x.train, y.train)
svcPreds <- as.matrix(svc$predict(x.test))
table(predicted = svcPreds, actual = PD.test$Class)
table(predicted = svcPreds, actual = y.test)
View(svcPreds)
svm(Class ~., data = PD.train, na.action = na.pass)
svm(Class ~., data = PD.train, na.action = na.omit)
svc <- svm(Class ~., data = PD.train, na.action = na.omit)
svcPred <- predict(svm, PD.test)
svcPred <- stats::predict(svm, PD.test)
svcPred <- predict(svc, PD.test)
svcPred
table(predicted = svcPred, actual = PD.test$Class)
table(predicted = svcPred, actual = na.omit(PD.test)$Class)
View(svc)
pdDT <- tree(Class~., data = PD.train)
plot(pdDT)
text(pdDT)
summary(PD)
svc <- svm(Class ~., data = PD.train, na.action = na.omit)
#get predictions
svcPred <- predict(svc, PD.test)
#confusion table
table(predicted = svcPred, actual = na.omit(PD.test)$Class)
nbTrain <- PD.train
nbTest <- PD.test
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
pdNB <- naiveBayes(Class~., data = nbTrain)
str(pdNB)
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
pdNBSimple <- naiveBayes(Class~., data = nbTrain)
pdNBSimplePredict <- predict(pdNBSimple, PD.test)
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
pdNBSimple <- naiveBayes(Class~., data = nbTrain)
pdNBSimplePredict <- predict(pdNBSimple, PD.test)
#confusion matrix
table(predicted = pdNBSimplePredict, actual = PD.test$Class)
pdNBSimple$tables
pdNBSimple$apriori
summary(nbTrain)
sapply(nbTrain, sd, na.rm = TRUE)
sapply(PD, sd, na.rm = TRUE)
library(tree)
library(e1071)
library(adabag)
library(rpart)
library(randomForest)
library(ROCR)
library(dplyr)
library(neuralnet)
sapply(PD, sd, na.rm = TRUE)
sapply(PD, sd)
apply(PD, sd, na.rm = TRUE)
sapply(df, sd, na.rm = TRUE)
sapply(PD, sd, na.rm = TRUE)
sapply(nbTrain[,-c(5)], sd, na.rm = TRUE)
pdNBSimple$tables
summary(nbTrain)
quantile(nbTrain$A01)
quantile(nbTrain$A01, probs = c(0, 0.25, 0.5, 0.75, 1))
quantile(nbTrain$A18, probs = c(0, 0.25, 0.5, 0.75, 1))
quantile(nbTrain$A18, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)
convertQuartile <- function(x) {
quartiles <- quantile(x, probs = c(0, 0.25, 0.5, 0.75, 1))
cut(x, breaks = quartiles, labels = c("1", "2", "3", "4"))
}
data[, c(1:4)] <- lapply(data[, c(1:4)], convertQuartile)
convertQuartile <- function(x) {
quartiles <- quantile(x, probs = c(0, 0.25, 0.5, 0.75, 1), na.action = na.pass)
cut(x, breaks = quartiles, labels = c("1", "2", "3", "4"))
}
data[, c(1:4)] <- lapply(data[, c(1:4)], convertQuartile)
pdNBSimple$tables
cut(nbTrain, breaks = 2)
cut(nbTrain$A01, breaks = 2)
summary(PD.train)
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
median(nbTrain$A18)
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTrain <- na.omit(nbTrain)
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
nbTest <- na.omit(nbTest)
median(nbTrain$A18)
pdNBSimple <- naiveBayes(Class~., data = nbTrain)
pdNBSimplePredict <- predict(pdNBSimple, PD.test)
pdNBSimple$tables
table(predicted = pdNBSimplePredict, actual = PD.test$Class)
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medainVal, 0, 1)
return(newCol)
}
nbTrain <- lapply(nbTrain, medianFactor)
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medianVal, 0, 1)
return(newCol)
}
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medianVal, 0, 1)
return(newCol)
}
nbTrain <- lapply(nbTrain, medianFactor)
nbTrain[, c(1:4)] <- lapply(nbTrain[, c(1:4)], medianFactor)
View(nbTrain)
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medianVal, 0, 1)
factor(newCol, levels = c(0, 1), labels = c(0, 1))
return(newCol)
}
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTrain <- na.omit(nbTrain)
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
nbTest <- na.omit(nbTest)
nbTrain[, c(1:4)] <- lapply(nbTrain[, c(1:4)], medianFactor)
View(nbTrain)
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTrain <- na.omit(nbTrain)
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
nbTest <- na.omit(nbTest)
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medianVal, 0, 1)
newCol <- factor(newCol, levels = c(0, 1), labels = c(0, 1))
return(newCol)
}
nbTrain[, c(1:4)] <- lapply(nbTrain[, c(1:4)], medianFactor)
View(nbTrain)
#create train and test datasets
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTrain <- na.omit(nbTrain)
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
nbTest <- na.omit(nbTest)
#splitting and factoring function
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medianVal, 0, 1)
newCol <- factor(newCol, levels = c(0, 1), labels = c(0, 1))
return(newCol)
}
#apply function to datasets
nbTrain[, c(1:4)] <- lapply(nbTrain[, c(1:4)], medianFactor)
nbTest[, c(1:4)] <- lapply(nbTest[, c(1:4)], medianFactor)
View(nbTest)
View(nbTrain)
pdNBSimple <- naiveBayes(Class~., data = nbTrain)
#make predictions
pdNBSimplePredict <- predict(pdNBSimple, PD.test)
pdNBSimple <- naiveBayes(Class~., data = nbTrain)
pdNBSimplePredict <- predict(pdNBSimple, nbTest)
table(predicted = pdNBSimplePredict, actual = PD.test$Class)
table(predicted = pdNBSimplePredict, actual = nbTest$Class)
pdNBSimple$tables
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTrain <- na.omit(nbTrain)
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
nbTest <- na.omit(nbTest)
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medianVal, 0, 1)
newCol <- factor(newCol, levels = c(0, 1), labels = c("Below", "Above or Equal"))
return(newCol)
}
nbTrain[, c(1:4)] <- lapply(nbTrain[, c(1:4)], medianFactor)
nbTest[, c(1:4)] <- lapply(nbTest[, c(1:4)], medianFactor)
View(nbTest)
View(nbTrain)
View(nbTest)
pdNBSimple <- naiveBayes(Class~., data = nbTrain)
pdNBSimple$tables
pdNBSimple$apriori
summary(PD.testOmit)
summary(nbTrain)
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTrain <- na.omit(nbTrain)
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
nbTest <- na.omit(nbTest)
summary(nbTrain)
nbTrain <- PD.train[, c(1, 18, 22, 23, 26)]
nbTrain <- na.omit(nbTrain)
nbTest <- PD.test[, c(1, 18, 22, 23, 26)]
nbTest <- na.omit(nbTest)
#summary table to view median values
summary(nbTrain)
#splitting and factoring function
medianFactor <- function(x) {
medianVal <- median(x)
newCol <- ifelse(x < medianVal, 0, 1)
newCol <- factor(newCol, levels = c(0, 1), labels = c("Below", "Above or Equal"))
return(newCol)
}
#apply function to datasets
nbTrain[, c(1:4)] <- lapply(nbTrain[, c(1:4)], medianFactor)
nbTest[, c(1:4)] <- lapply(nbTest[, c(1:4)], medianFactor)
pdNBSimple <- naiveBayes(Class~., data = nbTrain)
#model probability tables
pdNBSimple$tables
#model class distribution
pdNBSimple$apriori
#make predictions
pdNBSimplePredict <- predict(pdNBSimple, nbTest)
#confusion matrix
table(predicted = pdNBSimplePredict, actual = nbTest$Class)
pdNBSimplePrediction <- predict(pdNBSimple, nbTest, type = 'raw')[,2] %>%
prediction(., nbTest$Class)
pdNBSimplePrediction <- predict(pdNBSimple, nbTest, type = 'raw') %>%
prediction(., nbTest$Class)
pdNBSimplePrediction <- predict(pdNBSimple, nbTest, type = 'raw')
pdNBSimplePrediction
pdNBSimplePrediction <- predict(pdNBSimple, nbTest, type = 'raw') %>%
ROCR::prediction(., nbTest$Class)
pdNBSimplePrediction <- predict(pdNBSimple, nbTest, type = 'raw')[,2] %>%
ROCR::prediction(., nbTest$Class)
pdNBSimplePrediction
performance(pdNBSimplePrediction, "auc")@y.values
